{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# SQL related packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "# sklearn packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression as Log\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "# text analysis packages\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access to sql database\n",
    "dbname = 'patent_db'\n",
    "username = 'jy'\n",
    "pswd = 'jy'\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "# reading from sql database\n",
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12033, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from 2004-2007\n",
    "years = np.arange(2004, 2008)\n",
    "\n",
    "# dataframe to store the results\n",
    "claims = pd.DataFrame()\n",
    "\n",
    "# import the abstract from each table\n",
    "for year in years:\n",
    "    # query:\n",
    "    sql_query = \"\"\"\n",
    "    SELECT claims, id, payment_times\n",
    "        FROM patents_%s;\n",
    "    \"\"\" %str(year)\n",
    "\n",
    "    results = pd.read_sql_query(sql_query,con)\n",
    "    \n",
    "    claims = pd.concat([claims, results], axis = 0)\n",
    "    \n",
    "# check size of the data\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of patents with > 1 maintenance fee payments:  0.628521565694\n"
     ]
    }
   ],
   "source": [
    "# extract the response variable\n",
    "# reformat the response variable into binary\n",
    "y_data = np.zeros(abstracts.shape[0])\n",
    "y_data[abstracts['payment_times'].values >= 2] = 1\n",
    "\n",
    "print \"Percentage of patents with > 1 maintenance fee payments: \", np.mean(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_cleaning(text):\n",
    "    # tokenize the text first\n",
    "    tokens = word_tokenize(text.decode('utf-8'))\n",
    "    \n",
    "    # lowercase all the words\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # clean up stop words and punctuations \n",
    "    stop_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "    tokens_no_stop = [token for token in tokens\n",
    "                        if token not in stop_list]\n",
    "    \n",
    "    # extract stem of the words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens_no_stop]\n",
    "    \n",
    "    return tokens_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize_clean the abstracts and count the occurence of the words\n",
    "\n",
    "cleaned_text = []\n",
    "for i in range(claims.shape[0]):\n",
    "    tokens = tokenize_cleaning(claims['claims'].iloc[i])\n",
    "    cleaned_text.append(' '.join(word for word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to bag-of-words\n",
    "# min number selected by examine the 'number' words\n",
    "vectorizer = CountVectorizer(max_df = 0.4, min_df=140)\n",
    "\n",
    "# performe a count-based vectorization of the document\n",
    "word_vec = vectorizer.fit(cleaned_text)\n",
    "word_counts = word_vec.transform(cleaned_text)\n",
    "\n",
    "word_counts = word_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12033, 769)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 708.,   42.,   12.,    2.,    1.,    0.,    2.,    0.,    1.,    1.]),\n",
       " array([   224. ,   4552.1,   8880.2,  13208.3,  17536.4,  21864.5,\n",
       "         26192.6,  30520.7,  34848.8,  39176.9,  43505. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2dd1aea210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the distribution of word occurance\n",
    "total_counts = np.sum(word_counts, axis = 0)\n",
    "plt.hist(np.transpose(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'word': word_vec.get_feature_names(),\n",
    "    'count': total_counts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>712</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>952</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>748</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>723</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>920</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>644</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>526</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>650</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>443</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1035</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>342</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>398</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>322</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>443</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>551</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>251</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>251</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>259</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>251</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>602</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>224</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>278</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>432</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>754</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>350</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>345</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>648</td>\n",
       "      <td>absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1414</td>\n",
       "      <td>acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>507</td>\n",
       "      <td>accordance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1202</td>\n",
       "      <td>according</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>11172</td>\n",
       "      <td>acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1352</td>\n",
       "      <td>acids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>935</td>\n",
       "      <td>across</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>388</td>\n",
       "      <td>activated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1878</td>\n",
       "      <td>activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2423</td>\n",
       "      <td>adapted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1055</td>\n",
       "      <td>adding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>923</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2398</td>\n",
       "      <td>adjacent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>776</td>\n",
       "      <td>adjusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1729</td>\n",
       "      <td>administering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3004</td>\n",
       "      <td>agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1245</td>\n",
       "      <td>air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>457</td>\n",
       "      <td>algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>611</td>\n",
       "      <td>aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1133</td>\n",
       "      <td>alkyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>499</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count           word\n",
       "0    2022             10\n",
       "1     712            100\n",
       "2    1015             11\n",
       "3     952             12\n",
       "4     748             13\n",
       "5     723             14\n",
       "6     920             15\n",
       "7     644             16\n",
       "8     526             17\n",
       "9     650             18\n",
       "10    443             19\n",
       "11   1035             20\n",
       "12    342             21\n",
       "13    398             22\n",
       "14    322             23\n",
       "15    443             24\n",
       "16    551             25\n",
       "17    251             26\n",
       "18    251             27\n",
       "19    259             28\n",
       "20    251             29\n",
       "21    602             30\n",
       "22    224             32\n",
       "23    278             35\n",
       "24    432             40\n",
       "25    754             50\n",
       "26    350             90\n",
       "27    345             95\n",
       "28    648        absence\n",
       "29   1414     acceptable\n",
       "30    507     accordance\n",
       "31   1202      according\n",
       "32  11172           acid\n",
       "33   1352          acids\n",
       "34    935         across\n",
       "35    388      activated\n",
       "36   2022         active\n",
       "37   1878       activity\n",
       "38   2423        adapted\n",
       "39   1055         adding\n",
       "40    923     additional\n",
       "41   2398       adjacent\n",
       "42    776      adjusting\n",
       "43   1729  administering\n",
       "44   3004          agent\n",
       "45   1245            air\n",
       "46    457      algorithm\n",
       "47    611        aligned\n",
       "48   1133          alkyl\n",
       "49    499          allow"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12033, 29829)\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer()\n",
    "tfidf_article = tf_vectorizer.fit_transform(cleaned_text)\n",
    "tfidf_article = tfidf_article.toarray()\n",
    "\n",
    "print tfidf_article.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting train-test data and subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "x_train:  (9626, 769)\n",
      "x_test:  (2407, 769)\n",
      "y_train:  (9626,)\n",
      "y_test:  (2407,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(word_counts, y_data, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print \"Dataset dimensions:\"\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7136, 769)\n",
      "(7136,)\n"
     ]
    }
   ],
   "source": [
    "### subsampling the training data\n",
    "# sample the same number of'useful' patents as the 'not useful' patents\n",
    "# size of each class\n",
    "num_size = np.sum(y_train == 0)\n",
    "\n",
    "#random shuffle the rows\n",
    "n = x_train.shape[0]\n",
    "perm = range(n)\n",
    "np.random.shuffle(perm)\n",
    "\n",
    "x_train = x_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# separate the two classes\n",
    "x_useful = x_train[y_train == 1, :]\n",
    "x_not_useful = x_train[y_train == 0, :]\n",
    "y_useful = y_train[y_train == 1]\n",
    "y_not_useful = y_train[y_train == 0]\n",
    "\n",
    "# sample num_size from the 'useful' class\n",
    "x_useful = x_useful[:num_size]\n",
    "y_useful = y_useful[:num_size]\n",
    "\n",
    "# combine the two classes\n",
    "x_train_sub = np.concatenate((x_useful, x_not_useful), axis = 0)\n",
    "y_train_sub = np.concatenate((y_useful, y_not_useful), axis = 0)\n",
    "\n",
    "# shuffle again\n",
    "# shuffle the combined data\n",
    "n2 = x_train_sub.shape[0]\n",
    "perm2 = range(n2)\n",
    "np.random.shuffle(perm2)\n",
    "\n",
    "x_train_sub = x_train_sub[perm2]\n",
    "y_train_sub = y_train_sub[perm2]\n",
    "\n",
    "# check the size\n",
    "print x_train_sub.shape\n",
    "print y_train_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try Naive Bayes with Gaussian Distribution\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# fit on the training data\n",
    "gnb.fit(x_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict on the test data\n",
    "y_pred = gnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45201495637723305"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 752,  150],\n",
       "       [1169,  336]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try dimensionality reduction using PCA\n",
    "pca = PCA()\n",
    "\n",
    "x_train_pca = pca.fit_transform(x_train_sub)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the cum-variance explained at each level\n",
    "total_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_pc = np.where((total_var > 0.9) == True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3700"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### use logistic regression\n",
    "\n",
    "# call the model function\n",
    "model = Log()\n",
    "# parameter tuning\n",
    "c =  np.logspace(-5, 5, 11)\n",
    "\n",
    "# use grid search with 5-fold CV\n",
    "grid_model = GridSearchCV(model, param_grid = {'C': c}, cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_sub, y_train_sub) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.553391255605\n",
      "Best parameter:  {'C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.552139592854\n",
      "F1 score:  0.611111111111\n",
      "Precision:  0.562790697674\n",
      "Recall:  0.668508287293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[482, 420],\n",
       "       [658, 847]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_log = grid_model.best_estimator_\n",
    "best_log.fit(x_train_sub, y_train_sub)\n",
    "y_pred = best_log.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_pred, y_test)\n",
    "print \"Precision: \", metrics.precision_score(y_pred, y_test)\n",
    "print \"Recall: \", metrics.recall_score(y_pred, y_test)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### tune random forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 15, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "# grid_model = grid_model.fit(x_train_pca[:, :n_pc], y_train_sub)\n",
    "grid_model = grid_model.fit(x_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.560397982063\n",
      "Best parameter:  {'max_features': 6}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.570004154549\n",
      "F1 score:  0.62969588551\n",
      "Precision:  0.682170542636\n",
      "Recall:  0.584717607973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[492, 410],\n",
       "       [625, 880]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_sub, y_train_sub)\n",
    "y_pred = best_rf.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
