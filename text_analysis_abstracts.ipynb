{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# SQL related packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "# sklearn packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression as Log\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "# text analysis packages\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access to sql database\n",
    "dbname = 'patent_db'\n",
    "username = 'jy'\n",
    "pswd = 'jy'\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "# reading from sql database\n",
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract abstracts first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12033, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from 2004-2007\n",
    "years = np.arange(2004, 2008)\n",
    "\n",
    "# dataframe to store the results\n",
    "abstracts = pd.DataFrame()\n",
    "\n",
    "# import the abstract from each table\n",
    "for year in years:\n",
    "    # query:\n",
    "    sql_query = \"\"\"\n",
    "    SELECT abstract, id, payment_times\n",
    "        FROM patents_%s;\n",
    "    \"\"\" %str(year)\n",
    "\n",
    "    results = pd.read_sql_query(sql_query,con)\n",
    "    \n",
    "    abstracts = pd.concat([abstracts, results], axis = 0)\n",
    "    \n",
    "# check size of the data\n",
    "abstracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of patents with > 1 maintenance fee payments:  0.628521565694\n"
     ]
    }
   ],
   "source": [
    "# extract the response variable\n",
    "# reformat the response variable into binary\n",
    "y_data = np.zeros(abstracts.shape[0])\n",
    "y_data[abstracts['payment_times'].values >= 2] = 1\n",
    "\n",
    "print \"Percentage of patents with > 1 maintenance fee payments: \", np.mean(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_cleaning(text):\n",
    "    # tokenize the text first\n",
    "    tokens = word_tokenize(text.decode('utf-8'))\n",
    "    \n",
    "    # lowercase all the words\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # clean up stop words and punctuations \n",
    "    stop_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "    tokens_no_stop = [token for token in tokens\n",
    "                        if token not in stop_list]\n",
    "    \n",
    "    # extract stem of the words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens_no_stop]\n",
    "    \n",
    "    return tokens_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize_clean the abstracts and count the occurence of the words\n",
    "\n",
    "cleaned_text = []\n",
    "for i in range(abstracts.shape[0]):\n",
    "    tokens = tokenize_cleaning(abstracts['abstract'].iloc[i])\n",
    "    cleaned_text.append(' '.join(word for word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to bag-of-words\n",
    "# min number selected by examine the 'number' words\n",
    "vectorizer = CountVectorizer(max_df = 0.5, min_df=90)\n",
    "\n",
    "# performe a count-based vectorization of the document\n",
    "word_vec = vectorizer.fit(cleaned_text)\n",
    "word_counts = word_vec.transform(cleaned_text)\n",
    "\n",
    "word_counts = word_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12033, 1023)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 925.,   63.,   19.,    9.,    2.,    2.,    1.,    1.,    0.,    1.]),\n",
       " array([   92. ,   913.9,  1735.8,  2557.7,  3379.6,  4201.5,  5023.4,\n",
       "         5845.3,  6667.2,  7489.1,  8311. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhl6SOmfoJalzhl6SOvd/gvz6tepQumoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdeb6522c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the distribution of word occurance\n",
    "total_counts = np.sum(word_counts, axis = 0)\n",
    "plt.hist(np.transpose(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'word': word_vec.get_feature_names(),\n",
    "    'count': total_counts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>441</td>\n",
       "      <td>virus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>210</td>\n",
       "      <td>vitro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>348</td>\n",
       "      <td>vivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>819</td>\n",
       "      <td>voltage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>306</td>\n",
       "      <td>volume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>399</td>\n",
       "      <td>wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>686</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>334</td>\n",
       "      <td>wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>377</td>\n",
       "      <td>waveguide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>459</td>\n",
       "      <td>wavelength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>166</td>\n",
       "      <td>wavelengths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>113</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>313</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1006</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>171</td>\n",
       "      <td>whereby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1665</td>\n",
       "      <td>wherein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>186</td>\n",
       "      <td>whether</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>103</td>\n",
       "      <td>whose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>199</td>\n",
       "      <td>wide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>145</td>\n",
       "      <td>wild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1182</td>\n",
       "      <td>within</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>500</td>\n",
       "      <td>without</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>148</td>\n",
       "      <td>yield</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count         word\n",
       "1000    441        virus\n",
       "1001    210        vitro\n",
       "1002    348         vivo\n",
       "1003    819      voltage\n",
       "1004    306       volume\n",
       "1005    399         wall\n",
       "1006    686        water\n",
       "1007    334         wave\n",
       "1008    377    waveguide\n",
       "1009    459   wavelength\n",
       "1010    166  wavelengths\n",
       "1011    113          way\n",
       "1012    313       weight\n",
       "1013   1006         well\n",
       "1014    171      whereby\n",
       "1015   1665      wherein\n",
       "1016    186      whether\n",
       "1017    103        whose\n",
       "1018    199         wide\n",
       "1019    145         wild\n",
       "1020   1182       within\n",
       "1021    500      without\n",
       "1022    148        yield"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12033, 29829)\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer()\n",
    "tfidf_article = tf_vectorizer.fit_transform(cleaned_text)\n",
    "tfidf_article = tfidf_article.toarray()\n",
    "\n",
    "print tfidf_article.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting train-test data and subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "x_train:  (9626, 1023)\n",
      "x_test:  (2407, 1023)\n",
      "y_train:  (9626,)\n",
      "y_test:  (2407,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(word_counts, y_data, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print \"Dataset dimensions:\"\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7162, 1023)\n",
      "(7162,)\n"
     ]
    }
   ],
   "source": [
    "### subsampling the training data\n",
    "# sample the same number of'useful' patents as the 'not useful' patents\n",
    "# size of each class\n",
    "num_size = np.sum(y_train == 0)\n",
    "\n",
    "#random shuffle the rows\n",
    "n = x_train.shape[0]\n",
    "perm = range(n)\n",
    "np.random.shuffle(perm)\n",
    "\n",
    "x_train = x_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# separate the two classes\n",
    "x_useful = x_train[y_train == 1, :]\n",
    "x_not_useful = x_train[y_train == 0, :]\n",
    "y_useful = y_train[y_train == 1]\n",
    "y_not_useful = y_train[y_train == 0]\n",
    "\n",
    "# sample num_size from the 'useful' class\n",
    "x_useful = x_useful[:num_size]\n",
    "y_useful = y_useful[:num_size]\n",
    "\n",
    "# combine the two classes\n",
    "x_train_sub = np.concatenate((x_useful, x_not_useful), axis = 0)\n",
    "y_train_sub = np.concatenate((y_useful, y_not_useful), axis = 0)\n",
    "\n",
    "# shuffle again\n",
    "# shuffle the combined data\n",
    "n2 = x_train_sub.shape[0]\n",
    "perm2 = range(n2)\n",
    "np.random.shuffle(perm2)\n",
    "\n",
    "x_train_sub = x_train_sub[perm2]\n",
    "y_train_sub = y_train_sub[perm2]\n",
    "\n",
    "# check the size\n",
    "print x_train_sub.shape\n",
    "print y_train_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try Naive Bayes with Gaussian Distribution\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# fit on the training data\n",
    "gnb.fit(x_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict on the test data\n",
    "y_pred = gnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49148317407561282"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[367, 522],\n",
       "       [605, 913]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try dimensionality reduction using PCA\n",
    "pca = PCA()\n",
    "\n",
    "x_train_pca = pca.fit_transform(x_train_sub)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the cum-variance explained at each level\n",
    "total_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_pc = np.where((total_var > 0.9) == True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3700"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### use logistic regression\n",
    "\n",
    "# call the model function\n",
    "model = Log()\n",
    "# parameter tuning\n",
    "c =  np.logspace(-5, 5, 11)\n",
    "\n",
    "# use grid search with 5-fold CV\n",
    "grid_model = GridSearchCV(model, param_grid = {'C': c}, cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_pca[:, :n_pc], y_train_sub) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.547691457009\n",
      "Best parameter:  {'C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.524304113004\n",
      "F1 score:  0.552559593591\n",
      "Precision:  0.454954954955\n",
      "Recall:  0.703482587065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[555, 298],\n",
       "       [847, 707]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_log = grid_model.best_estimator_\n",
    "best_log.fit(x_train_pca, y_train_sub)\n",
    "y_pred = best_log.predict(x_test_pca)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_pred, y_test)\n",
    "print \"Precision: \", metrics.precision_score(y_pred, y_test)\n",
    "print \"Recall: \", metrics.recall_score(y_pred, y_test)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### tune random forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 15, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "# grid_model = grid_model.fit(x_train_pca[:, :n_pc], y_train_sub)\n",
    "grid_model = grid_model.fit(x_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.5048869031\n",
      "Best parameter:  {'max_features': 12}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.495637723307\n",
      "F1 score:  0.554004408523\n",
      "Precision:  0.626245847176\n",
      "Recall:  0.496706192358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[439, 450],\n",
       "       [764, 754]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_sub, y_train_sub)\n",
    "y_pred = best_rf.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try word2vec, meaning on the document level;\n",
    "can combine text features with non-text features\n",
    "\n",
    "\n",
    "## NLP: \n",
    "text to number;\n",
    "\n",
    "### BAG-OF-WORDS approach:\n",
    "TF-IDF: scale the numbers;\n",
    "you can hard code some key words as features, otherwise IF-IDF may decrease performance.\n",
    "\n",
    "**Topic modeling**: only at the document level\n",
    "Each document is a distribution over topics; each topic is a distribution over terms/words.\n",
    "Matrix: rows are documents, columns are topics, row sum up to 1.\n",
    "Topics: matirx as well. But you need to assign topic label to train the model. Need to set the number of topics. \n",
    "Basic model: LDA (latent Dirichlet)\n",
    "\n",
    "### WORD2VEC (word embedding, there is also GLOVE)\n",
    "Gensin is the library to use!\n",
    "spxy (harder to learn)\n",
    "NLTK (not great but easy to use)\n",
    "\n",
    "continuous bag-of-words, or the other method to predict the word\n",
    "\n",
    "there are pretrained models on Google news or wikipedia\n",
    "\n",
    "Can do manual inspection to check the models\n",
    "\n",
    "features: numbers representing the word/doc meaning \n",
    "\n",
    "to documents: take sum or mean as the features\n",
    "\n",
    "**Doc2vec**: document level, or summarize the key words and do word2vec\n",
    "\n",
    "shuffle the order of sentenses and train the model again, if corpus is very small. \n",
    "\n",
    "can pick up spelling problems.\n",
    "\n",
    "Gensin takes list of lists\n",
    "\n",
    "PROCESSES:\n",
    "smaller corpus requires more preprocessing;\n",
    "1. split sentenses/words\n",
    "2. stemming (large documents do not care much)\n",
    "3. stop words\n",
    "4. punctuation: ', \", \n",
    "5. numbers (no need to remove does not need to touch it)\n",
    "6. lowercase everything\n",
    "\n",
    "other features to consider: how long the patent is. it is easier to interpret. \n",
    "\n",
    "tSNE is preferred over PCA on text analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940, 1023)\n",
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "### subsampling the entire data\n",
    "# sample the same number of'useful' patents as the 'not useful' patents\n",
    "# size of each class\n",
    "num_size = np.sum(y_data == 0)\n",
    "\n",
    "#random shuffle the rows\n",
    "n = word_counts.shape[0]\n",
    "perm = range(n)\n",
    "np.random.shuffle(perm)\n",
    "\n",
    "x_data = word_counts[perm]\n",
    "y_data = y_data[perm]\n",
    "\n",
    "# separate the two classes\n",
    "x_useful = x_data[y_data == 1, :]\n",
    "x_not_useful = x_data[y_data == 0, :]\n",
    "y_useful = y_data[y_data == 1]\n",
    "y_not_useful = y_data[y_data == 0]\n",
    "\n",
    "# sample num_size from the 'useful' class\n",
    "x_useful = x_useful[:num_size]\n",
    "y_useful = y_useful[:num_size]\n",
    "\n",
    "# combine the two classes\n",
    "x_data_sub = np.concatenate((x_useful, x_not_useful), axis = 0)\n",
    "y_data_sub = np.concatenate((y_useful, y_not_useful), axis = 0)\n",
    "\n",
    "# shuffle again\n",
    "# shuffle the combined data\n",
    "n2 = x_data_sub.shape[0]\n",
    "perm2 = range(n2)\n",
    "np.random.shuffle(perm2)\n",
    "\n",
    "x_data_sub = x_data_sub[perm2]\n",
    "y_data_sub = y_data_sub[perm2]\n",
    "\n",
    "# check the size\n",
    "print x_data_sub.shape\n",
    "print y_data_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "x_train:  (7152, 1023)\n",
      "x_test:  (1788, 1023)\n",
      "y_train:  (7152,)\n",
      "y_test:  (1788,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data_sub, y_data_sub, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print \"Dataset dimensions:\"\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### tune random forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 15, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "# grid_model = grid_model.fit(x_train_pca[:, :n_pc], y_train_sub)\n",
    "grid_model = grid_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.574804250559\n",
      "Best parameter:  {'max_features': 4}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.579977628635\n",
      "F1 score:  0.580681183696\n",
      "Precision:  0.572057205721\n",
      "Recall:  0.589569160998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[517, 389],\n",
       "       [362, 520]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train, y_train)\n",
    "y_pred = best_rf.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
