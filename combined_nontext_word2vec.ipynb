{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# SQL related packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "# sklearn packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.linear_model import LogisticRegression as Log\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "# text analysis packages\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import word2vec, Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load non-text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data with the non-text features\n",
    "patents = pd.read_pickle(\"patent_data/nontext_features.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12030, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>num_applications</th>\n",
       "      <th>num_patent_citations</th>\n",
       "      <th>num_nonpatent_citations</th>\n",
       "      <th>num_claims</th>\n",
       "      <th>num_similar_doc</th>\n",
       "      <th>num_authors</th>\n",
       "      <th>payment_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US6699658B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US6699724B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>47</td>\n",
       "      <td>44</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US6690816B2</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US6711436B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>109</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US6711432B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>114</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  publication_year  B  C  D  E  F  G  H  num_applications  \\\n",
       "0  US6699658B1              2004  0  1  0  0  0  0  0                 5   \n",
       "1  US6699724B1              2004  0  0  0  0  0  1  0                32   \n",
       "2  US6690816B2              2004  0  0  0  0  0  1  0                 8   \n",
       "3  US6711436B1              2004  0  0  0  0  0  0  0                 4   \n",
       "4  US6711432B1              2004  0  0  0  0  0  0  0                 7   \n",
       "\n",
       "   num_patent_citations  num_nonpatent_citations  num_claims  num_similar_doc  \\\n",
       "0                    28                       34          42                1   \n",
       "1                    47                       44          25                0   \n",
       "2                     9                        0          32                1   \n",
       "3                   105                      109          45                7   \n",
       "4                    15                      114          44                3   \n",
       "\n",
       "   num_authors  payment_times  \n",
       "0            4              3  \n",
       "1            4              3  \n",
       "2            4              1  \n",
       "3            1              3  \n",
       "4            4              3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print patents.shape\n",
    "patents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of patents with > 1 maintenance fee payments:  0.62859517872\n"
     ]
    }
   ],
   "source": [
    "# reformat the response variable into binary\n",
    "y_data = np.zeros(patents.shape[0])\n",
    "y_data[patents['payment_times'].values >= 2] = 1\n",
    "\n",
    "print \"Percentage of patents with > 1 maintenance fee payments: \", np.mean(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictors\n",
    "x_data = patents.drop(['id', 'payment_times', 'publication_year'], axis = 1).values\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load claims data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# access to sql database\n",
    "dbname = 'patent_db'\n",
    "username = 'jy'\n",
    "pswd = 'jy'\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "# reading from sql database\n",
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12033, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from 2004-2007\n",
    "years = np.arange(2004, 2008)\n",
    "\n",
    "# dataframe to store the results\n",
    "claims = pd.DataFrame()\n",
    "\n",
    "# import the abstract from each table\n",
    "for year in years:\n",
    "    # query:\n",
    "    sql_query = \"\"\"\n",
    "    SELECT claims, id, payment_times, classification\n",
    "        FROM patents_%s;\n",
    "    \"\"\" %str(year)\n",
    "\n",
    "    results = pd.read_sql_query(sql_query,con)\n",
    "    \n",
    "    claims = pd.concat([claims, results], axis = 0)\n",
    "    \n",
    "# check size of the data\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the patents missing classification data\n",
    "missing_class_index = (claims['classification'].values == 'NA')\n",
    "\n",
    "# reassign patent index\n",
    "claims.index = range(len(claims.index))\n",
    "# drop the rows\n",
    "claims =  claims.drop(patents.index[missing_class_index])\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_cleaning(text):\n",
    "    # tokenize the text first\n",
    "    tokens = word_tokenize(text.decode('utf-8'))\n",
    "    \n",
    "    # lowercase all the words\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # clean up stop words and punctuations \n",
    "    stop_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "    tokens_no_stop = [token for token in tokens\n",
    "                        if token not in stop_list]            \n",
    "    \n",
    "#     # extract stem of the words\n",
    "#     stemmer = PorterStemmer()\n",
    "#     tokens_stem = [stemmer.stem(token) for token in tokens_no_stop]\n",
    "\n",
    "    # use lemma instead\n",
    "    # reason: remove the influence of plural or tense\n",
    "    # but retain the subtle difference in legal writting\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemma = [lemmatizer.lemmatize(token) for token in tokens_no_stop]\n",
    "    \n",
    "    # remove numbers (the actual values are not useful)\n",
    "    tokens_no_num = []\n",
    "    for token in tokens_lemma:\n",
    "        try:\n",
    "            float(token)\n",
    "        except:\n",
    "            tokens_no_num.append(token)\n",
    "    \n",
    "    return tokens_no_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize_clean the claims and count the occurence of the words\n",
    "\n",
    "cleaned_text = []\n",
    "for i in range(claims.shape[0]):\n",
    "    tokens = tokenize_cleaning(claims['claims'].iloc[i])\n",
    "    cleaned_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load('models/word2vec_claims_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute average word-vector for a text\n",
    "def dec_vec(model, text):\n",
    "    # store the vector for each word\n",
    "    vectors = []\n",
    "    \n",
    "    # compute on each word\n",
    "    for j in range(len(text)):\n",
    "        try:\n",
    "            vectors.append(model.wv[text[j]])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not vectors:\n",
    "        vectors_mean = np.zeros((1, 100))\n",
    "    else:\n",
    "        vectors_mean = np.nanmean(vectors, axis = 0)\n",
    "        vectors_mean = vectors_mean.reshape((1, 100))\n",
    "    \n",
    "    # return vector mean\n",
    "    return vectors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute a vector for each patent\n",
    "claims_vec = np.zeros((1, 100)) #initialize\n",
    "\n",
    "for i in range(claims.shape[0]):\n",
    "    vec = dec_vec(word2vec_model, cleaned_text[i])\n",
    "    claims_vec = np.concatenate([claims_vec, vec], axis = 0)\n",
    "\n",
    "# remove the first line\n",
    "claims_vec = claims_vec[1:, :]\n",
    "claims_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine text and non-text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the claims data with the non-text features\n",
    "x_data = np.concatenate([x_data, claims_vec], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "x_train:  (10030, 113)\n",
      "x_test:  (2000, 113)\n",
      "y_train:  (10030,)\n",
      "y_test:  (2000,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, \n",
    "                                                    test_size = 2000, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print \"Dataset dimensions:\"\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7506, 113)\n",
      "(7506,)\n"
     ]
    }
   ],
   "source": [
    "### subsampling the training data\n",
    "# sample the same number of'useful' patents as the 'not useful' patents\n",
    "# size of each class\n",
    "num_size = np.sum(y_train == 0)\n",
    "\n",
    "#random shuffle the rows\n",
    "n = x_train.shape[0]\n",
    "perm = range(n)\n",
    "np.random.shuffle(perm)\n",
    "\n",
    "x_train = x_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# separate the two classes\n",
    "x_useful = x_train[y_train == 1, :]\n",
    "x_not_useful = x_train[y_train == 0, :]\n",
    "y_useful = y_train[y_train == 1]\n",
    "y_not_useful = y_train[y_train == 0]\n",
    "\n",
    "# sample num_size from the 'useful' class\n",
    "x_useful = x_useful[:num_size]\n",
    "y_useful = y_useful[:num_size]\n",
    "\n",
    "# combine the two classes\n",
    "x_train_sub = np.concatenate((x_useful, x_not_useful), axis = 0)\n",
    "y_train_sub = np.concatenate((y_useful, y_not_useful), axis = 0)\n",
    "\n",
    "# shuffle again\n",
    "# shuffle the combined data\n",
    "n2 = x_train_sub.shape[0]\n",
    "perm2 = range(n2)\n",
    "np.random.shuffle(perm2)\n",
    "\n",
    "x_train_sub = x_train_sub[perm2]\n",
    "y_train_sub = y_train_sub[perm2]\n",
    "\n",
    "# check the size\n",
    "print x_train_sub.shape\n",
    "print y_train_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # standardize the predictors\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# x_train_std = scaler.fit_transform(x_train_sub)\n",
    "# x_test_std = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the saved scaler\n",
    "scaler = pickle.load(open('models/final_model_scaler.p', 'r'))\n",
    "x_test_std = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write my own scoring function\n",
    "def my_loss_func(y_actual, y_pred):\n",
    "    \n",
    "    # set the price of applying for each patent\n",
    "    patent_cost = 20\n",
    "    \n",
    "    # set the value of a useful patent\n",
    "    useful_value = 30\n",
    "    \n",
    "    total_cost = np.sum(y_pred == 1) * patent_cost\n",
    "    total_value = np.sum((y_actual == 1) & (y_pred == 1)) * useful_value\n",
    "    \n",
    "    profit = total_value -  total_cost\n",
    "    \n",
    "    return profit\n",
    "\n",
    "my_scorer = make_scorer(my_loss_func, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing weighted average F1 score for the two classes\n",
    "def average_f1_score(y_actual, y_pred):\n",
    "    # for class 1\n",
    "    f1_1 = metrics.f1_score(y_actual, y_pred)\n",
    "    # for class 0\n",
    "    y_actual_flip = np.zeros(len(y_actual))\n",
    "    y_actual_flip[y_actual == 0] = 1\n",
    "    y_pred_flip = np.zeros(len(y_pred))\n",
    "    y_pred_flip[y_pred == 0] = 1\n",
    "    f1_2 = metrics.f1_score(y_actual_flip, y_pred_flip)\n",
    "    \n",
    "    # weigthed average\n",
    "    f1 = (f1_1 * np.sum(y_actual == 1) + f1_2 * np.sum(y_actual == 0))/len(y_actual)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing weighted average precision for the two classes\n",
    "def average_precision(y_actual, y_pred):\n",
    "    # for class 1\n",
    "    precision_1 = metrics.precision_score(y_actual, y_pred)\n",
    "    # for class 0\n",
    "    y_actual_flip = np.zeros(len(y_actual))\n",
    "    y_actual_flip[y_actual == 0] = 1\n",
    "    y_pred_flip = np.zeros(len(y_pred))\n",
    "    y_pred_flip[y_pred == 0] = 1\n",
    "    precision_2 = metrics.precision_score(y_actual_flip, y_pred_flip)\n",
    "    \n",
    "    # weigthed average\n",
    "    precision = (precision_1 * np.sum(y_actual == 1) + \n",
    "          precision_2 * np.sum(y_actual == 0))/len(y_actual)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing weighted average recall for the two classes\n",
    "def average_recall(y_actual, y_pred):\n",
    "    # for class 1\n",
    "    recall_1 = metrics.recall_score(y_actual, y_pred)\n",
    "    # for class 0\n",
    "    y_actual_flip = np.zeros(len(y_actual))\n",
    "    y_actual_flip[y_actual == 0] = 1\n",
    "    y_pred_flip = np.zeros(len(y_pred))\n",
    "    y_pred_flip[y_pred == 0] = 1\n",
    "    recall_2 = metrics.recall_score(y_actual_flip, y_pred_flip)\n",
    "    \n",
    "    # weigthed average\n",
    "    recall = (recall_1 * np.sum(y_actual == 1) + \n",
    "          recall_2 * np.sum(y_actual == 0))/len(y_actual)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest: final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### tune random forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 10, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = my_scorer)\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_std, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: -1113.03756994\n",
      "Best parameter:  {'max_features': 6}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved final model\n",
    "best_rf = pickle.load(open('models/final_model.p', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test accuracy:  0.569\n",
      "Average F1 score:  0.560065410581\n",
      "Average Precision:  0.574924040162\n",
      "Average Recall:  0.569\n",
      "Net value:  1340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[426, 289],\n",
       "       [573, 712]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # check confusion matrix\n",
    "# best_rf = grid_model.best_estimator_\n",
    "# best_rf.fit(x_train_std, y_train_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_std)\n",
    "\n",
    "# model performance\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"Average F1 score: \", average_f1_score(y_pred, y_test)\n",
    "print \"Average Precision: \", average_precision(y_pred, y_test)\n",
    "print \"Average Recall: \", average_recall(y_pred, y_test)\n",
    "print \"Net value: \", my_loss_func(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional tuning (not used in the final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tune class weight of random forest\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 8, 2)\n",
    "\n",
    "# add class weight tuning to the random forest model\n",
    "weights = np.logspace(-3,3,7)\n",
    "weight_list_dict = [{0:1, 1: weights[i]} for i in range(len(weights))]\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space,\n",
    "                                       'class_weight': weight_list_dict}, \n",
    "                          cv  = 5, scoring = my_scorer)\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_std, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: -913.073541167\n",
      "Best parameter:  {'max_features': 2, 'class_weight': {0: 1, 1: 1000.0}}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5095\n",
      "F1 score:  0.526772793054\n",
      "Precision:  0.692893401015\n",
      "Recall:  0.424902723735\n",
      "Net value:  310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[473, 242],\n",
       "       [739, 546]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_std, y_train_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_std)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "print \"Net value: \", my_loss_func(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check where the model can get if selected by accuracy\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 10, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_std, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.569411137756\n",
      "Best parameter:  {'max_features': 8}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_std, y_train_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_std)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_pred, y_test)\n",
    "print \"Precision: \", metrics.precision_score(y_pred, y_test)\n",
    "print \"Recall: \", metrics.recall_score(y_pred, y_test)\n",
    "print \"Net value: \", my_loss_func(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6425\n",
      "Average F1 score:  0.502656012177\n",
      "Average Precision:  0.41280625\n",
      "Average Recall:  0.6425\n",
      "Net value:  -1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jy/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,  715],\n",
       "       [   0, 1285]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if everything is predicted to be useful (benchmark)\n",
    "y_pred_uniform = np.ones(y_pred.shape)\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred_uniform == y_test)\n",
    "print \"Average F1 score: \", average_f1_score(y_test, y_pred_uniform)\n",
    "print \"Average Precision: \", average_precision(y_test, y_pred_uniform)\n",
    "print \"Average Recall: \", average_recall(y_test, y_pred_uniform)\n",
    "print \"Net value: \", my_loss_func(y_test, y_pred_uniform)\n",
    "metrics.confusion_matrix(y_test, y_pred_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.547\n",
      "Average F1 score:  0.546716637733\n",
      "Average Precision:  0.546437337899\n",
      "Average Recall:  0.547\n",
      "Net value:  -760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[260, 455],\n",
       "       [451, 834]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if randomly assign the two classes with the same ratio\n",
    "y_pred_random = np.random.uniform(0, 1, size = y_test.shape[0])\n",
    "# turn into binary\n",
    "y_random_class = np.zeros(y_test.shape[0]) \n",
    "y_random_class[y_pred_random > 1- np.mean(y_test)] = 1\n",
    "\n",
    "print \"Test accuracy: \", np.mean(y_random_class == y_test)\n",
    "print \"Average F1 score: \", average_f1_score(y_test, y_random_class)\n",
    "print \"Average Precision: \", average_precision(y_test, y_random_class)\n",
    "print \"Average Recall: \", average_recall(y_test, y_random_class)\n",
    "print \"Net value: \", my_loss_func(y_test, y_random_class)\n",
    "metrics.confusion_matrix(y_test, y_random_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
