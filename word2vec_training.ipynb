{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# SQL related packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "# sklearn packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.linear_model import LogisticRegression as Log\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "# text analysis packages\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access to sql database\n",
    "dbname = 'patent_db'\n",
    "username = 'jy'\n",
    "pswd = 'jy'\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "# reading from sql database\n",
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12033, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from 2004-2007\n",
    "years = np.arange(2004, 2008)\n",
    "\n",
    "# dataframe to store the results\n",
    "claims = pd.DataFrame()\n",
    "\n",
    "# import the abstract from each table\n",
    "for year in years:\n",
    "    # query:\n",
    "    sql_query = \"\"\"\n",
    "    SELECT claims, id, payment_times, classification\n",
    "        FROM patents_%s;\n",
    "    \"\"\" %str(year)\n",
    "\n",
    "    results = pd.read_sql_query(sql_query,con)\n",
    "    \n",
    "    claims = pd.concat([claims, results], axis = 0)\n",
    "    \n",
    "# check size of the data\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of patents with > 1 maintenance fee payments:  0.628521565694\n"
     ]
    }
   ],
   "source": [
    "# extract the response variable\n",
    "# reformat the response variable into binary\n",
    "y_data = np.zeros(claims.shape[0])\n",
    "y_data[claims['payment_times'].values >= 2] = 1\n",
    "\n",
    "print \"Percentage of patents with > 1 maintenance fee payments: \", np.mean(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the patents missing classification data\n",
    "missing_class_index = (claims['classification'].values == 'NA')\n",
    "\n",
    "# reassign patent index\n",
    "claims.index = range(len(claims.index))\n",
    "# drop the rows\n",
    "claims =  claims.drop(claims.index[missing_class_index])\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = y_data[claims['classification'].values != 'NA']\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "x_train:  (10030, 4)\n",
      "x_test:  (2000, 4)\n",
      "y_train:  (10030,)\n",
      "y_test:  (2000,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(claims, y_data[:claims.shape[0]], \n",
    "                                                    test_size = 2000, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print \"Dataset dimensions:\"\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_cleaning(text):\n",
    "    # tokenize the text first\n",
    "    tokens = word_tokenize(text.decode('utf-8'))\n",
    "    \n",
    "    # lowercase all the words\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # clean up stop words and punctuations \n",
    "    stop_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "    tokens_no_stop = [token for token in tokens\n",
    "                        if token not in stop_list]            \n",
    "    \n",
    "#     # extract stem of the words\n",
    "#     stemmer = PorterStemmer()\n",
    "#     tokens_stem = [stemmer.stem(token) for token in tokens_no_stop]\n",
    "\n",
    "    # use lemma instead\n",
    "    # reason: remove the influence of plural or tense\n",
    "    # but retain the subtle difference in legal writting\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemma = [lemmatizer.lemmatize(token) for token in tokens_no_stop]\n",
    "    \n",
    "    # remove numbers (the actual values are not useful)\n",
    "    tokens_no_num = []\n",
    "    for token in tokens_lemma:\n",
    "        try:\n",
    "            float(token)\n",
    "        except:\n",
    "            tokens_no_num.append(token)\n",
    "    \n",
    "    return tokens_no_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize_clean the data for word2vec training\n",
    "\n",
    "cleaned_text = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    tokens = tokenize_cleaning(x_train['claims'].iloc[i])\n",
    "    cleaned_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(cleaned_text, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'polypeptide', 0.7180240154266357),\n",
       " (u'mutant', 0.6947092413902283),\n",
       " (u'subunit', 0.6833655834197998),\n",
       " (u'kinase', 0.6764980554580688),\n",
       " (u'receptor', 0.6745332479476929),\n",
       " (u'gfp', 0.669427752494812),\n",
       " (u'translocation', 0.6464871764183044),\n",
       " (u'chimeric', 0.6401758193969727),\n",
       " (u'pi3', 0.6384416222572327),\n",
       " (u'app', 0.6349186897277832)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['protein'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the word2vec model\n",
    "model.save('models/word2vec_claims_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the word2vec model\n",
    "model = Word2Vec.load('models/word2vec_claims_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize_clean for training the final model\n",
    "\n",
    "cleaned_text = []\n",
    "for i in range(claims.shape[0]):\n",
    "    tokens = tokenize_cleaning(claims['claims'].iloc[i])\n",
    "    cleaned_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute average word-vector for a text\n",
    "def dec_vec(model, text):\n",
    "    # store the vector for each word\n",
    "    vectors = []\n",
    "    \n",
    "    # compute on each word\n",
    "    for j in range(len(text)):\n",
    "        try:\n",
    "            vectors.append(model.wv[text[j]])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not vectors:\n",
    "        vectors_mean = np.zeros((1, 100))\n",
    "    else:\n",
    "        vectors_mean = np.nanmean(vectors, axis = 0)\n",
    "        vectors_mean = vectors_mean.reshape((1, 100))\n",
    "    \n",
    "    # return vector mean\n",
    "    return vectors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "claims_vec = np.zeros((1, 100))\n",
    "\n",
    "for i in range(claims.shape[0]):\n",
    "    vec = dec_vec(model, cleaned_text[i])\n",
    "    claims_vec = np.concatenate([claims_vec, vec], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030, 100)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_vec = claims_vec[1:, :]\n",
    "claims_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "x_train:  (10030, 100)\n",
      "x_test:  (2000, 100)\n",
      "y_train:  (10030,)\n",
      "y_test:  (2000,)\n"
     ]
    }
   ],
   "source": [
    "# split the training and testing data\n",
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(claims_vec, y_data, \n",
    "                                                    test_size = 2000, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print \"Dataset dimensions:\"\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8217, 100)\n",
      "(8217,)\n"
     ]
    }
   ],
   "source": [
    "# sample the same number of'useful' patents as the 'not useful' patents\n",
    "# size of each class\n",
    "num_size = np.sum(y_data == 0)\n",
    "\n",
    "#random shuffle the rows\n",
    "n = x_train.shape[0]\n",
    "perm = range(n)\n",
    "np.random.shuffle(perm)\n",
    "\n",
    "x_train = x_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# separate the two classes\n",
    "x_useful = x_train[y_train == 1, :]\n",
    "x_not_useful = x_train[y_train == 0, :]\n",
    "y_useful = y_train[y_train == 1]\n",
    "y_not_useful = y_train[y_train == 0]\n",
    "\n",
    "# sample num_size from the 'useful' class\n",
    "x_useful = x_useful[:num_size]\n",
    "y_useful = y_useful[:num_size]\n",
    "\n",
    "# combine the two classes\n",
    "x_data_sub = np.concatenate((x_useful, x_not_useful), axis = 0)\n",
    "y_data_sub = np.concatenate((y_useful, y_not_useful), axis = 0)\n",
    "\n",
    "# shuffle again\n",
    "# shuffle the combined data\n",
    "n2 = x_data_sub.shape[0]\n",
    "perm2 = range(n2)\n",
    "np.random.shuffle(perm2)\n",
    "\n",
    "x_data_sub = x_data_sub[perm2]\n",
    "y_data_sub = y_data_sub[perm2]\n",
    "\n",
    "# check the size\n",
    "print x_data_sub.shape\n",
    "print y_data_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize the predictors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "x_train_std = scaler.fit_transform(x_data_sub)\n",
    "x_test_std = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### tune random forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 8, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_std, y_data_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.545332846538\n",
      "Best parameter:  {'max_features': 4}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5835\n",
      "F1 score:  0.687664041995\n",
      "Precision:  0.660662824207\n",
      "Recall:  0.716966379984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[250, 471],\n",
       "       [362, 917]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_std, y_data_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_std)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write my own scoring function\n",
    "def my_loss_func(y_actual, y_pred):\n",
    "    \n",
    "    # set the price of applying for each patent\n",
    "    patent_cost = 10\n",
    "    \n",
    "    # set the value of a useful patent\n",
    "    useful_value = 15\n",
    "    \n",
    "    total_cost = np.sum(y_pred == 1) * patent_cost\n",
    "    total_value = np.sum((y_actual == 1) & (y_pred == 1)) * useful_value\n",
    "    \n",
    "    profit = total_value -  total_cost\n",
    "    \n",
    "    return profit\n",
    "\n",
    "my_scorer = make_scorer(my_loss_func, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### tune random forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 8, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = my_scorer)\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_std, y_data_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: -1590.00669344\n",
      "Best parameter:  {'max_features': 6}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5805\n",
      "F1 score:  0.687523277467\n",
      "Precision:  0.656472261735\n",
      "Recall:  0.721657544957\n",
      "Net value:  -215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[238, 483],\n",
       "       [356, 923]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_std, y_data_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_std)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "print \"Net value: \", my_loss_func(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tune class weight of random forest\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 8, 2)\n",
    "\n",
    "# add class weight tuning to the random forest model\n",
    "weights = np.logspace(-3,3,7)\n",
    "weight_list_dict = [{0:1, 1: weights[i]} for i in range(len(weights))]\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space,\n",
    "                                       'class_weight': weight_list_dict}, \n",
    "                          cv  = 5, scoring = my_scorer)\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_std, y_data_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: -1243.99537544\n",
      "Best parameter:  {'max_features': 2, 'class_weight': {0: 1, 1: 1000.0}}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.535\n",
      "F1 score:  0.610878661088\n",
      "Precision:  0.657065706571\n",
      "Recall:  0.570758405004\n",
      "Net value:  -160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[340, 381],\n",
       "       [549, 730]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_std, y_data_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_std)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "print \"Net value: \", my_loss_func(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6395\n",
      "F1 score:  0.780115888991\n",
      "Precision:  0.6395\n",
      "Recall:  1.0\n",
      "Net value:  -815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,  721],\n",
       "       [   0, 1279]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if everything is predicted to be useful (benchmark)\n",
    "y_pred_uniform = np.ones(y_pred.shape)\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred_uniform == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred_uniform)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred_uniform)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred_uniform)\n",
    "print \"Net value: \", my_loss_func(y_test, y_pred_uniform)\n",
    "metrics.confusion_matrix(y_test, y_pred_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.527\n",
      "F1 score:  0.627265563436\n",
      "Precision:  0.632247815727\n",
      "Recall:  0.622361219703\n",
      "Net value:  -650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[258, 463],\n",
       "       [483, 796]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if randomly assign the two classes with the same ratio\n",
    "y_pred_random = np.random.uniform(0, 1, size = y_test.shape[0])\n",
    "# turn into binary\n",
    "y_random_class = np.zeros(y_test.shape[0]) \n",
    "y_random_class[y_pred_random > 1- np.mean(y_test)] = 1\n",
    "\n",
    "print \"Test accuracy: \", np.mean(y_random_class == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_random_class, y_test)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_random_class)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_random_class)\n",
    "print \"Net value: \", my_loss_func(y_test, y_random_class)\n",
    "metrics.confusion_matrix(y_test, y_random_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
