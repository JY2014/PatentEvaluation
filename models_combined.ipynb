{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "# SQL related packages\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "# sklearn packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.linear_model import LogisticRegression as Log\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "# text analysis packages\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the non-text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data with the non-text features\n",
    "patents = pd.read_pickle(\"patent_data/nontext_features.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12030, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>num_applications</th>\n",
       "      <th>num_patent_citations</th>\n",
       "      <th>num_nonpatent_citations</th>\n",
       "      <th>num_claims</th>\n",
       "      <th>num_similar_doc</th>\n",
       "      <th>num_authors</th>\n",
       "      <th>payment_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US6699658B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US6699724B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>47</td>\n",
       "      <td>44</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US6690816B2</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US6711436B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>109</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US6711432B1</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>114</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  publication_year  B  C  D  E  F  G  H  num_applications  \\\n",
       "0  US6699658B1              2004  0  1  0  0  0  0  0                 5   \n",
       "1  US6699724B1              2004  0  0  0  0  0  1  0                32   \n",
       "2  US6690816B2              2004  0  0  0  0  0  1  0                 8   \n",
       "3  US6711436B1              2004  0  0  0  0  0  0  0                 4   \n",
       "4  US6711432B1              2004  0  0  0  0  0  0  0                 7   \n",
       "\n",
       "   num_patent_citations  num_nonpatent_citations  num_claims  num_similar_doc  \\\n",
       "0                    28                       34          42                1   \n",
       "1                    47                       44          25                0   \n",
       "2                     9                        0          32                1   \n",
       "3                   105                      109          45                7   \n",
       "4                    15                      114          44                3   \n",
       "\n",
       "   num_authors  payment_times  \n",
       "0            4              3  \n",
       "1            4              3  \n",
       "2            4              1  \n",
       "3            1              3  \n",
       "4            4              3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print patents.shape\n",
    "patents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of patents with > 1 maintenance fee payments:  0.62859517872\n"
     ]
    }
   ],
   "source": [
    "# reformat the response variable into binary\n",
    "y_data = np.zeros(patents.shape[0])\n",
    "y_data[patents['payment_times'].values >= 2] = 1\n",
    "\n",
    "print \"Percentage of patents with > 1 maintenance fee payments: \", np.mean(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictors\n",
    "x_data = patents.drop(['id', 'payment_times', 'publication_year'], axis = 1).values\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load claims data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#access to sql database\n",
    "dbname = 'patent_db'\n",
    "username = 'jy'\n",
    "pswd = 'jy'\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "# reading from sql database\n",
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12033, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from 2004-2007\n",
    "years = np.arange(2004, 2008)\n",
    "\n",
    "# dataframe to store the results\n",
    "claims = pd.DataFrame()\n",
    "\n",
    "# import the abstract from each table\n",
    "for year in years:\n",
    "    # query:\n",
    "    sql_query = \"\"\"\n",
    "    SELECT claims, id, payment_times, classification\n",
    "        FROM patents_%s;\n",
    "    \"\"\" %str(year)\n",
    "\n",
    "    results = pd.read_sql_query(sql_query,con)\n",
    "    \n",
    "    claims = pd.concat([claims, results], axis = 0)\n",
    "    \n",
    "# check size of the data\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/pandas/indexes/base.py:1434: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 12030 but corresponding boolean dimension is 12033\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12030, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the patents missing classification data\n",
    "missing_class_index = (claims['classification'].values == 'NA')\n",
    "\n",
    "# reassign patent index\n",
    "claims.index = range(len(claims.index))\n",
    "# drop the rows\n",
    "claims =  claims.drop(patents.index[missing_class_index])\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_cleaning(text):\n",
    "    # tokenize the text first\n",
    "    tokens = word_tokenize(text.decode('utf-8'))\n",
    "    \n",
    "    # lowercase all the words\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # clean up stop words and punctuations \n",
    "    stop_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "    tokens_no_stop = [token for token in tokens\n",
    "                        if token not in stop_list]            \n",
    "    \n",
    "#     # extract stem of the words\n",
    "#     stemmer = PorterStemmer()\n",
    "#     tokens_stem = [stemmer.stem(token) for token in tokens_no_stop]\n",
    "\n",
    "    # use lemma instead\n",
    "    # reason: remove the influence of plural or tense\n",
    "    # but retain the subtle difference in legal writting\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemma = [lemmatizer.lemmatize(token) for token in tokens_no_stop]\n",
    "    \n",
    "    # remove numbers (the actual values are not useful)\n",
    "    tokens_no_num = []\n",
    "    for token in tokens_lemma:\n",
    "        try:\n",
    "            float(token)\n",
    "        except:\n",
    "            tokens_no_num.append(token)\n",
    "    \n",
    "    return tokens_no_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize_clean the claims and count the occurence of the words\n",
    "\n",
    "cleaned_text = []\n",
    "for i in range(claims.shape[0]):\n",
    "    tokens = tokenize_cleaning(claims['claims'].iloc[i])\n",
    "    cleaned_text.append(' '.join(word for word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12030, 8034)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to bag-of-words\n",
    "# min number selected by examining the low-frequency words\n",
    "vectorizer = CountVectorizer(max_df = 0.4, min_df=5)\n",
    "\n",
    "# perform a count-based vectorization of the document\n",
    "word_vec = vectorizer.fit(cleaned_text)\n",
    "word_counts = word_vec.transform(cleaned_text)\n",
    "\n",
    "# convert to array\n",
    "word_counts = word_counts.toarray()\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numerical words detected:  222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12030, 7812)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still need to remove some number words \n",
    "# due to how CountVectorizer treats '-' and '/'\n",
    "\n",
    "# remove any word with numbers in it\n",
    "words = word_vec.get_feature_names()\n",
    "num_word_index = np.zeros(len(words))\n",
    "\n",
    "for i in range(len(words)):\n",
    "    word = words[i]\n",
    "    for j in range(len(word)):\n",
    "        try:\n",
    "            float(word[j])\n",
    "            num_word_index[i] = 1\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "print \"Number of numerical words detected: \", int(np.sum(num_word_index))\n",
    "\n",
    "# remove the number words\n",
    "words_no_num = np.asarray(words)[num_word_index == 0]\n",
    "word_counts = word_counts[:, num_word_index == 0]\n",
    "\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the claims data with the non-text features\n",
    "x_data = np.concatenate([x_data, word_counts], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "x_train:  (9624, 7825)\n",
      "x_test:  (2406, 7825)\n",
      "y_train:  (9624,)\n",
      "y_test:  (2406,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print \"Dataset dimensions:\"\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using subsampling to generate balanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7198, 7825)\n",
      "(7198,)\n"
     ]
    }
   ],
   "source": [
    "### subsampling the training data\n",
    "# sample the same number of'useful' patents as the 'not useful' patents\n",
    "# size of each class\n",
    "num_size = np.sum(y_train == 0)\n",
    "\n",
    "#random shuffle the rows\n",
    "n = x_train.shape[0]\n",
    "perm = range(n)\n",
    "np.random.shuffle(perm)\n",
    "\n",
    "x_train = x_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# separate the two classes\n",
    "x_useful = x_train[y_train == 1, :]\n",
    "x_not_useful = x_train[y_train == 0, :]\n",
    "y_useful = y_train[y_train == 1]\n",
    "y_not_useful = y_train[y_train == 0]\n",
    "\n",
    "# sample num_size from the 'useful' class\n",
    "x_useful = x_useful[:num_size]\n",
    "y_useful = y_useful[:num_size]\n",
    "\n",
    "# combine the two classes\n",
    "x_train_sub = np.concatenate((x_useful, x_not_useful), axis = 0)\n",
    "y_train_sub = np.concatenate((y_useful, y_not_useful), axis = 0)\n",
    "\n",
    "# shuffle again\n",
    "# shuffle the combined data\n",
    "n2 = x_train_sub.shape[0]\n",
    "perm2 = range(n2)\n",
    "np.random.shuffle(perm2)\n",
    "\n",
    "x_train_sub = x_train_sub[perm2]\n",
    "y_train_sub = y_train_sub[perm2]\n",
    "\n",
    "# check the size\n",
    "print x_train_sub.shape\n",
    "print y_train_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# standardize the predictors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "x_train_std = scaler.fit_transform(x_train_sub)\n",
    "x_test_std = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on the claims word-counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try dimensionality reduction using PCA\n",
    "pca = PCA()\n",
    "\n",
    "x_train_counts_pca = pca.fit_transform(x_train_std[:, 13:])\n",
    "x_test_counts_pca = pca.transform(x_test_std[:, 13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of PCs that can explain > 90% variability:  2425\n"
     ]
    }
   ],
   "source": [
    "# find the cum-variance explained at each level\n",
    "total_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_pc = np.where((total_var > 0.9) == True)[0][0]\n",
    "\n",
    "print \"The number of PCs that can explain > 90% variability: \", n_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine to the non-text features\n",
    "x_train_pca = np.concatenate([x_train_std[:, :13], x_train_counts_pca[:, :n_pc]], axis = 1)\n",
    "x_test_pca = np.concatenate([x_test_std[:, :13], x_test_counts_pca[:, :n_pc]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### use logistic regression\n",
    "\n",
    "# call the model function\n",
    "model = Log()\n",
    "# parameter tuning\n",
    "c =  np.logspace(-5, 5, 11)\n",
    "\n",
    "# use grid search with 5-fold CV\n",
    "grid_model = GridSearchCV(model, param_grid = {'C': c}, cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "grid_model.fit(x_train_pca, y_train_sub) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.564184495693\n",
      "Best parameter:  {'C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.536575228595\n",
      "F1 score:  0.552747693542\n",
      "Precision:  0.720711297071\n",
      "Recall:  0.448275862069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[602, 267],\n",
       "       [848, 689]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_log = grid_model.best_estimator_\n",
    "best_log.fit(x_train_pca, y_train_sub)\n",
    "\n",
    "y_pred = best_log.predict(x_test_pca)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Log()\n",
    "\n",
    "# parameter tuning\n",
    "c =  np.logspace(-5, 3, 9)\n",
    "\n",
    "# add class weight tuning to the random forest model\n",
    "weights = np.logspace(-4,0,5)\n",
    "weight_list_dict = [{0:1, 1: weights[i]} for i in range(len(weights))]\n",
    "\n",
    "# use grid search with 5-fold CV\n",
    "grid_model = GridSearchCV(model, param_grid = {'C': c,'class_weight': weight_list_dict}, \n",
    "                          cv  = 5, scoring = 'f1')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_pca, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.528970792818\n",
      "Best parameter:  {'C': 1.0, 'class_weight': {0: 1, 1: 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.540315876974\n",
      "F1 score:  0.586077844311\n",
      "Precision:  0.68986784141\n",
      "Recall:  0.509433962264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[517, 352],\n",
       "       [754, 783]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_log = grid_model.best_estimator_\n",
    "best_log.fit(x_train_pca, y_train_sub)\n",
    "\n",
    "y_pred = best_log.predict(x_test_pca)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### tune random forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 14, 2)\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space}, \n",
    "                          cv  = 5, scoring = 'accuracy')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_pca, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.550708530147\n",
      "Best parameter:  {'max_features': 2}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.531587697423\n",
      "F1 score:  0.580573129885\n",
      "Precision:  0.507482108003\n",
      "Recall:  0.678260869565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[499, 370],\n",
       "       [757, 780]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_pca, y_train_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_pca)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_pred, y_test)\n",
    "print \"Precision: \", metrics.precision_score(y_pred, y_test)\n",
    "print \"Recall: \", metrics.recall_score(y_pred, y_test)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tuning class weight\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 10, 2)\n",
    "\n",
    "# add class weight tuning to the random forest model\n",
    "weights = np.logspace(-4,0,5)\n",
    "weight_list_dict = [{0:1, 1: weights[i]} for i in range(len(weights))]\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space,\n",
    "                                       'class_weight': weight_list_dict}, \n",
    "                          cv  = 5, scoring = 'f1')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_pca, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.600771935735\n",
      "Best parameter:  {'max_features': 8, 'class_weight': {0: 1, 1: 0.001}}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.548628428928\n",
      "F1 score:  0.626033057851\n",
      "Precision:  0.664959765911\n",
      "Recall:  0.591411841249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[411, 458],\n",
       "       [628, 909]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_pca, y_train_sub)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_pca)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle the rf model\n",
    "pickle.dump(best_rf, open(\"models/best_rf_combined.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle the pca and scaler\n",
    "pickle.dump(pca, open(\"models/best_rf_pca.p\", 'wb'))\n",
    "pickle.dump(scaler, open(\"models/best_rf_scaler.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## try open the model and predict\n",
    "#rf = pickle.load(open(\"models/best_rf.p\", 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly work on the imbalanced training data + tune class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# standardize the predictors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "x_train_std = scaler.fit_transform(x_train)\n",
    "x_test_std = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try dimensionality reduction using PCA\n",
    "pca = PCA()\n",
    "\n",
    "x_train_counts_pca = pca.fit_transform(x_train_std[:, 13:])\n",
    "x_test_counts_pca = pca.transform(x_test_std[:, 13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of PCs that can explain > 90% variability:  2797\n"
     ]
    }
   ],
   "source": [
    "# find the cum-variance explained at each level\n",
    "total_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_pc = np.where((total_var > 0.9) == True)[0][0]\n",
    "\n",
    "print \"The number of PCs that can explain > 90% variability: \", n_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine to the non-text features\n",
    "x_train_pca = np.concatenate([x_train_std[:, :13], x_train_counts_pca[:, :n_pc]], axis = 1)\n",
    "x_test_pca = np.concatenate([x_test_std[:, :13], x_test_counts_pca[:, :n_pc]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest + tuning class weight\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# tune max_features\n",
    "param_space = np.arange(2, 8, 2)\n",
    "\n",
    "# add class weight tuning to the random forest model\n",
    "weights = np.logspace(-8,0,9)\n",
    "weight_list_dict = [{0:1, 1: weights[i]} for i in range(len(weights))]\n",
    "\n",
    "grid_model = GridSearchCV(model, n_jobs = 4, \n",
    "                          param_grid = {'max_features': param_space,\n",
    "                                       'class_weight': weight_list_dict}, \n",
    "                          cv  = 5, scoring = 'f1')\n",
    "# fit on the data\n",
    "grid_model = grid_model.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.768468109077\n",
      "Best parameter:  {'max_features': 2, 'class_weight': {0: 1, 1: 1.0000000000000001e-05}}\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print \"Best accuracy:\", grid_model.best_score_\n",
    "print \"Best parameter: \", grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.644222776392\n",
      "F1 score:  0.776384535005\n",
      "Precision:  0.648625054561\n",
      "Recall:  0.966818477554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  64,  805],\n",
       "       [  51, 1486]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check confusion matrix\n",
    "best_rf = grid_model.best_estimator_\n",
    "best_rf.fit(x_train_pca, y_train)\n",
    "\n",
    "y_pred = best_rf.predict(x_test_pca)\n",
    "\n",
    "# accuracy\n",
    "print \"Test accuracy: \", np.mean(y_pred == y_test)\n",
    "print \"F1 score: \", metrics.f1_score(y_test, y_pred)\n",
    "print \"Precision: \", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall: \", metrics.recall_score(y_test, y_pred)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
